
Use AdamW Optimizer for Training.
C:\Users\lvcha\miniconda3\envs\mmsd2\lib\site-packages\transformers\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
Epoch:   0%|                                                            | 0/10 [00:00<?, ?it/s]































































































































































































































12/24/2025 19:25:25 - INFO - root -   i_epoch is 0, dev_acc is 0.8390041493775934, dev_f1 is 0.8203703703703703, dev_precision is 0.7924865831842576, dev_recall is 0.8502879078694817
12/24/2025 19:26:32 - INFO - root -   i_epoch is 0, test_acc is 0.8484848484848485, macro_test_f1 is 0.8471634013285001, macro_test_precision is 0.8459846037794241, macro_test_recall is 0.8520309763249563, micro_test_f1 is 0.8329519450800916, micro_test_precision is 0.7926829268292683, micro_test_recall is 0.8775313404050145
Epoch:  10%|████▉                                            | 1/10 [09:08<1:22:15, 548.41s/it]





































































































































Epoch:  10%|████▉                                            | 1/10 [13:36<2:02:31, 816.86s/it]
Traceback (most recent call last):
  File "src/main.py", line 97, in <module>
    main()
  File "src/main.py", line 92, in main
    train(args, model, device, train_data, dev_data, test_data, processor)
  File "D:\users\lvcha\Documents\万恶之源研究生版\毕业相关\多模块\MMSD2.0\src\train.py", line 78, in train
    inputs = processor(text=text_list, images=image_list, padding='max_length', truncation=True, max_length=args.max_len, return_tensors="pt").to(device)
  File "C:\Users\lvcha\miniconda3\envs\mmsd2\lib\site-packages\transformers\models\clip\processing_clip.py", line 85, in __call__
    image_features = self.feature_extractor(images, return_tensors=return_tensors, **kwargs)
  File "C:\Users\lvcha\miniconda3\envs\mmsd2\lib\site-packages\transformers\models\clip\feature_extraction_clip.py", line 150, in __call__
    images = [self.convert_rgb(image) for image in images]
  File "C:\Users\lvcha\miniconda3\envs\mmsd2\lib\site-packages\transformers\models\clip\feature_extraction_clip.py", line 150, in <listcomp>
    images = [self.convert_rgb(image) for image in images]
  File "C:\Users\lvcha\miniconda3\envs\mmsd2\lib\site-packages\transformers\image_utils.py", line 132, in convert_rgb
    return image.convert("RGB")
  File "C:\Users\lvcha\miniconda3\envs\mmsd2\lib\site-packages\PIL\Image.py", line 933, in convert
    self.load()
  File "C:\Users\lvcha\miniconda3\envs\mmsd2\lib\site-packages\PIL\ImageFile.py", line 269, in load
    n, err_code = decoder.decode(b)
KeyboardInterrupt